{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A TensorFlow Word2Vec Model for Word Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Word2Vec is a model that was created by [Mikolov et al.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It uses the concept of \"word embeddings\", which is a way to represent relationships between words using vectors. This makes it a useful tool to find words that are similar to eachother.\n",
    "\n",
    "Here is an example of an embedding matrix taken from the TensorFlow tutorial:\n",
    "\n",
    "![embedding_matrix](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data used here is a cleaned version of the first 10^9 bytes of an English Wikipedia dump performed on Mar. 3, 2006.  See [this site](https://cs.fit.edu/~mmahoney/compression/textdata.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = maybe_download('text8.zip', url, 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = read_data(filename)\n",
    "print(vocabulary[:7])\n",
    "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(vocabulary_size=10000):\n",
    "    \"\"\"Read data and create the dictionary\"\"\"\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \"\"\"Generate batch data\"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = vocabulary_size     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.arange(valid_size) # np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a fast scheme called Noise Contrastive Estimation (NCE).  Instead of taking the probability of the context word compared to all of the possible context words in the vocabulary, this method randomly samples 2-20 possible context words and evaluates the probability only from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    " \n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "    \n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(graph, num_steps):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        with session.as_default():\n",
    "\n",
    "            # We must initialize all variables before we use them.\n",
    "            init.run()\n",
    "            print('Initialized')\n",
    "\n",
    "            average_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                batch_inputs, batch_context = generate_batch(data,\n",
    "                    batch_size, num_skips, skip_window)\n",
    "                feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "                # We perform one update step by evaluating the optimizer op (including it\n",
    "                # in the list of returned values for session.run()\n",
    "                _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "\n",
    "                if step % 1000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0\n",
    "\n",
    "                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                if step % 1000 == 0:\n",
    "                    sim = similarity.eval()\n",
    "                    for i in range(valid_size):\n",
    "                        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                        top_k = 8  # number of nearest neighbors\n",
    "                        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                        log_str = 'Nearest to %s:' % valid_word\n",
    "                        for k in range(top_k):\n",
    "                            close_word = reverse_dictionary[nearest[k]]\n",
    "                            log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "                    \n",
    "            final_embeddings = normalized_embeddings.eval()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, os.path.join(\"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  251.206222534\n",
      "Nearest to papua: justinian, roosevelt, pork, movements, discourse, moderate, maiden, wide,\n",
      "Average loss at step  1000 :  32.2978411992\n",
      "Nearest to papua: justinian, roosevelt, pork, discourse, maiden, movements, codified, translations,\n",
      "Average loss at step  2000 :  8.70521029985\n",
      "Nearest to papua: justinian, roosevelt, movements, discourse, moderate, pork, wide, built,\n",
      "Average loss at step  3000 :  4.89532488751\n",
      "Nearest to papua: justinian, roosevelt, movements, discourse, moderate, wide, pork, built,\n",
      "Average loss at step  4000 :  3.54418580842\n",
      "Nearest to papua: justinian, roosevelt, movements, discourse, moderate, wide, pork, built,\n",
      "Average loss at step  5000 :  2.91368192589\n",
      "Nearest to papua: justinian, roosevelt, movements, discourse, moderate, wide, pork, built,\n",
      "Average loss at step  6000 :  2.82110228467\n",
      "Nearest to papua: justinian, roosevelt, movements, discourse, moderate, wide, pork, maiden,\n",
      "Average loss at step  7000 :  2.61711239123\n",
      "Nearest to papua: justinian, roosevelt, movements, moderate, wide, built, discourse, attend,\n",
      "Average loss at step  8000 :  2.51311168265\n",
      "Nearest to papua: justinian, roosevelt, movements, moderate, discourse, built, wide, attend,\n",
      "Average loss at step  9000 :  2.42942111349\n",
      "Nearest to papua: justinian, roosevelt, movements, moderate, discourse, wide, built, attend,\n",
      "Training took 1.9453682333333333 minutes to run 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "softmax_start_time = dt.datetime.now()\n",
    "train(graph, num_steps=num_steps)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Training took {} minutes to run {} iterations\".format(\n",
    "    (softmax_end_time-softmax_start_time).total_seconds()/60, str(num_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_sim(input_word, model_path):\n",
    "    # Reinitialize things\n",
    "    with graph.as_default(): \n",
    "        \n",
    "        # Input data.\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "          normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "          valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session,\n",
    "                            os.path.join(model_path, \"model.ckpt\"))\n",
    "\n",
    "        sim = similarity.eval()\n",
    "        if input_word in dictionary:\n",
    "            idx = dictionary[input_word]\n",
    "            valid_word = reverse_dictionary[idx]\n",
    "            top_k = 3  # number of nearest neighbors\n",
    "            nearest = (-sim[idx, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "        else:\n",
    "            return 'Word not present in dictionary.  Try a different one.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the trained model and see if it can predict similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define location of saved model\n",
    "model_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/dave/DataScience/Projects/GitHub/pybotframework/tutorials/tensorflow_word2vec/model.ckpt\n",
      "Nearest to science: twelve woman christ\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "predict_sim('science', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises:\n",
    "1. Following this tutorial, create a TensorFlow model and train it.\n",
    "2. Using the model you created, adjust the hyperparatmeters and see if the model training improves.\n",
    "\n",
    "## Advanced Exercises:\n",
    "1. Download a separate dataset from the internet. Reformat so that it can be understood by TensorFlow. Train a new TensorFlow model and see if it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
